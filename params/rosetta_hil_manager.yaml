# Rosetta HIL Launch Configuration
#
# This file is the single source of truth for all defaults in rosetta_hil_launch.py.
# It has a top-level contract_path shared by all nodes, plus four sections:
#
#   hil_manager     - RosettaHilManagerNode parameters
#   robot_policy    - rosetta_client_node parameters (merged over rosetta_client.yaml)
#   episode_recorder - episode_recorder_node parameters (merged over episode_recorder.yaml)
#   reward_classifier - rosetta_client_node (classifier) parameters
#
# Any value set here overrides the corresponding node's own YAML defaults.
# CLI arguments override everything:
#
#   ros2 launch rosetta rosetta_hil_launch.py enable_reward_classifier:=false
#   ros2 launch rosetta rosetta_hil_launch.py pretrained_name_or_path:=/path/to/model

# Contract used by all nodes in this launch (hil_manager, robot_policy, episode_recorder,
# and reward_classifier all read from the same contract).
contract_path: "/workspaces/rosetta_ws/install/share/rosetta/contracts/so_101_hil.yaml"

# ==============================================================================
# HIL Manager
# ==============================================================================

hil_manager:
  ros__parameters:
    # Whether to use a reward classifier policy (requires separate contract + model)
    enable_reward_classifier: true

    # Action names for child node action clients
    # These must match the namespaced action names from the launch file
    policy_action_name: "/robot_policy/run_policy"
    reward_classifier_action_name: "/reward_classifier/run_policy"
    recorder_action_name: "/record_episode"

    # Topic prefix applied to remapped policy output
    # The robot policy rosetta_client_node publishes to {prefix}{original_topic}
    # The HIL manager subscribes there and forwards to the original topic
    policy_remap_prefix: "/hil/policy"

    # Topic prefix applied to remapped reward classifier output
    reward_remap_prefix: "/hil/reward"

    # Reward values assigned when the human presses override buttons
    human_reward_positive: 1.0
    human_reward_negative: 0.0

    # Rate at which to publish ManageEpisode feedback (Hz)
    feedback_rate_hz: 30.0

# ==============================================================================
# Robot Policy  (rosetta_client_node, namespace: robot_policy)
# Merged over rosetta_client.yaml — values here take precedence.
# ==============================================================================

robot_policy:
  ros__parameters:
    # Path to trained policy model (local path or HuggingFace repo_id)
    pretrained_name_or_path: "/workspaces/rosetta_ws/models/ACT/checkpoints/last/pretrained_model"

    # LeRobot policy server address (host:port)
    server_address: "127.0.0.1:8080"

    # Policy architecture type
    policy_type: "act"

    # Device for policy inference
    policy_device: "cuda"

    # Number of actions to request per inference chunk
    actions_per_chunk: 30

    # Threshold for when to request a new action chunk (0.0 - 1.0)
    chunk_size_threshold: 0.95

    # How to aggregate overlapping action chunks
    aggregate_fn_name: "weighted_average"

    # Observation similarity filtering tolerance (-1.0 to disable)
    obs_similarity_atol: -1.0

# ==============================================================================
# Episode Recorder  (episode_recorder_node)
# Merged over episode_recorder.yaml — values here take precedence.
# ==============================================================================

episode_recorder:
  ros__parameters:
    # Where to store recorded bag files
    bag_base_dir: "/workspaces/rosetta_ws/datasets/bags"

    # Rosbag storage format: "mcap" (recommended) or "sqlite3"
    storage_id: "mcap"

    # Default max episode duration in seconds
    default_max_duration: 300.0

# ==============================================================================
# Reward Classifier  (rosetta_client_node, namespace: reward_classifier)
# Only used when enable_reward_classifier: true.
# Starts from rosetta_client.yaml base, values here take precedence.
# ==============================================================================

reward_classifier:
  ros__parameters:
    # Path to trained reward classifier model (required when enabled)
    pretrained_name_or_path: "/workspaces/rosetta_ws/checkpoints/reward_classifier"

    # Policy server address for the reward classifier (separate port from robot policy)
    server_address: "127.0.0.1:8081"

    # Policy type for the reward classifier model
    policy_type: "reward_classifier"

    # Contract path for reward classifier (empty = use main contract_path)
    contract_path: ""
